%\setchapterpreamble[u]{}

\chapter{Concept} \label{chap:Concept}
\minitoc\vspace{1em}

\section{Background}
\todo{Intro in the concept}

\section{Graph Theory}
\subsection{Labeled Graphs}
Graph $G = (V,E)$ is a finite non-empty set of vertices $V = {v_{1}, v_{2}, …}$ with a set of edges $E = {e_{1}, e_{2}, …}$ that connect pairs of vertices.
If the edges have direction, then the $G$ is a directed graph (digraph). 
An ordered pair describes the directionality of an edge e in digraph $G: e = (u,v)$,
where $u$ is the starting vertex (source) and $v$ is the ending vertex (target). 

A labeled graph $G$ is a graph where vertices or edges are associated with labels \cite{Champin2003}. 
More precise, given a finite set of vertex labels $L_{V}$ and a finite set of edge labels $L_{E}$, a labeled graph is defined by a the tuple $G = (V, E, r_{V} , r_{E})$ such that:

\begin{itemize}
	\item[--]  $V$ is a finite set of vertices,
	\item[--] $r_{V} \subseteq V \times L_{V}$ is the function that assigns labels to vertices
	\item[--]  $r_{E} \subseteq V \times V \times L_{E}$ is the function that assigns labels to edges
\end{itemize}

The tuples from $r_{V}$ and $ r_{E} $ describe the vertex features and edge features of $G$ respectively. 

%For labeled directed graphs, the set of edges is now formally described with additional parameter $l$ that corresponds to the label:
%\begin{equation}
% E = \{(u, v) | \exists l, (u, v, l) \in r_{E}\}
%\end{equation}

\subsection{Node Ranking}
Ranking the nodes (vertices) reflects their importance within a graph. 
Based on the link structure, we estimate how relevant certain node is.
The very straightforward way for computing a rank is the degree of the node: the number of incoming and outgoing  edges from a node. However, this doesn’t propagate the score throughout the nodes.

To overcome this, we use the PageRank (PR) algorithm \cite{PageRank}. PR ranks all the node in a graph by forming a probability distribution i.e.\ the cumulative score of all PR scores sums up to one. 

PR will assign high score to a node if: (1) there are many nodes pointing to it and (2) those nodes also have high PR score.


\begin{equation}
	PR(X) = (1-d) + d(\dfrac{PR(V_{1})}{O(V_{1})} + ... + \dfrac{PR(V_{N})}{O(V_{N})})
\end{equation}
\todo{Explain PR algorithm}

\subsection{Graph Comparison}
\subsubsection{Isomorphism}
\subsubsection{Maximum Common Subgraph}
\subsubsection{Node Coverage}

\section{Similarity}
\subsection{Introduction}
The concept of similarity appears in almost every scientific domain. It is defined by similarity measure based on object representations.
Similarity measure (similarity function) is a function that quantifies how closely related are two objects. 
In other words, by using similarity as criterion, we can evaluate the likeliness between objects. 

If we consider the following spaces: 
\begin{itemize}
	\item[--]  $\mathds{X}$ denotes the set of the data objects
	\item[--]  $\mathds{F}$ denotes the feature space
	\item[--]  $\mathds{R} ^  \mathds{F}$ denotes the space of all feature representations
\end{itemize}

then a similarity model $S:\mathds{X} \times \mathds{X} \rightarrow :\mathds{R}$ defined $\forall x,y \in \mathds{X}$ as:
\begin{equation}
	S(x,y) = s(f_{x},f_{y})
\end{equation}

where $f$ 

Similarity function $s$ is subject to the following properties: \cite{Santini:1999:SM:317043.317048}:

\begin{itemize}
	\item Symmetry: $\forall x,y \in \mathds{X}: s(x,y) = s(y,x)$ - the order of the objects in the input should not affect the output score
	\item Maximum self-similarity: $\forall x,y \in \mathds{X}: s(x,x) \geqslant s(x,y)$ - nothing can be more similar than the object itself
\end{itemize}

For applying similarity function, an object representation is needed with a corresponding distance function. 
Object representation allows us to extract the features from the given data objects. 
Depending on the domain, different feature extraction methods can be applied. 

The distance function allows us to give score for the closeness between the corresponding features.
It directly corresponds to the dissimilarity measure: it gets value close to zero for objects which are similar (i.e. close) to each other. 
In other words, similar objects tend to have short distances between one another.

For function $d:\mathds{X} \times \mathds{X} \rightarrow :\mathds{R}$ to qualify as distance, in needs to fulfill the following constraints \ref{Deza.Deza2009EncyclopediaofDistances}:
\begin{itemize}
	\item Non-negativity: $\forall x,y \in \mathds{X}: d(x,y) \leqslant 0$
	\item Reflexivity: $\forall x \in \mathds{X}: d(x,x) = 0$
	\item Symmetry: $\forall x,y \in \mathds{X}: d(x,y) = d(y,x)$
\end{itemize}

Further more, a distance $d$ is a metric, if it additionally satisfies the following properties:
\begin{itemize}
	\item Identity of indiscernibles: $\forall x,y \in \mathds{X}: d(x,y) = 0 \Leftrightarrow x=y$
	\item Triangle inequality: $\forall x,y,z \in \mathds{X}: d(x,y) \leqslant d(x,z) + d(y,z)$
\end{itemize}

\subsubsection{Conversion form distance to similarity}

The conversion from distance to similarity may differ. 
Any monotonically decreasing transformation can be applied to convert similarity measures into dissimilarity measures, 
and any monotonically increasing transformation can be applied to convert the similarity to distance.
\todo{Cite lecture notes}\url{http://users.csc.calpoly.edu/~dekhtyar/560-Fall2009/lectures/lec09.466.pdf}

If the similarity values are normalized in the range from 0 to 1 (degree of similarity), then the corresponding dissimilarity can be expressed as : 
\begin{equation}
d(x,y) = 1 - s(x,y)
\label{disi}
\end{equation}

\subsection{Example distance and similarity functions  between attributes}
\subsubsection{Euclidean distance}

Euclidean distance measures the direct distance between two points in Euclidean space. For two d-dimensional points $x$ and $y$, the Euclidean distance is defined as:
\begin{equation}
	d^{EUC}(x,y) = \sqrt{(x_{1} - y_{1})^{2} + (x_{2} - y_{2})^{2} + ... + (x_{d} - y_{d})^{2} } = \sqrt{\sum_{i=1}^{d} (x_{i}-y_{i})^{2}}
\end{equation}

Norm of the vector.

\begin{equation}
	\lVert x\lVert = \sqrt{\sum_{i=1}^{d}x_{i}^{2}}
	\title{eq:norm}
\end{equation}

The Euclidean distance is a metric distance\cite{Deza.Deza2009EncyclopediaofDistances}.

\subsubsection{Cosine similarity}
The cosine similarity is defined through the dot product between vectors. For two vectors $x$ and $y$, the dot product is expressed as:

\begin{equation}
	x\cdot y^{T} = \lVert x\lVert \lVert y\lVert \cos{\theta}
	\label{eq:dot}
\end{equation}


where the $\lVert x\lVert$ and $\lVert y\lVert$ are the norms of the vectors x and y respectively), 
and $\theta$ is the angle between the two vectors.
The equation \ref{eq:dot} can be used to express the $\cos \theta$ value:

\begin{equation}
	\cos \theta = s^{COS} \dfrac{x \cdot y}{\lVert x\lVert \lVert y\lVert} = 
	\dfrac{\sum_{i=1}^{d}x_{i}y_{i}}{\sqrt{\sum_{i=1}^{d} x_{i}^{2}}\sqrt{\sum_{i=1}^{d} y_{i}^{2}}}
	\label{eq:cosine}
\end{equation} 

Equation \ref{eq:cosine} defines the cosine similarity function $s^{COS}$, which combines dot product (nominator) with normalization (denominator).

The cosine similarity looks into the angle between the vectors. 
For $\theta$ values close to zero, the cosine similarity will assign a value close to 1, which means that that the two vectors are very similar to each other. 
Although the cosine value ranges between -1 and 1, if we restrict the features only to positive values as shown in TF-IDF 
\todo{Refer to TF-IDF chapter}, the value of $\theta$ will range from 0 to 90 degrees. 
Consequently, the cosine similarity will range between 0 and 1.

This normalization allows us to define the Cosine distance as 
$d^{COS}1 - \cos \theta$. 
However, this is not a metric distance, as it does not fulfill the triangle inequality.

\subsubsection{Binary Similarity}
The binary similarity corresponds to exact matching. 

\begin{equation}
\forall x,y \in \mathds{X}: s^{BIN}(x,y)=
\begin{cases}
1  & \quad \text{if } x=y \\
0  & \quad \text{else }\\
\end{cases}
\label{eq:binary} 
\end{equation}

Although very restrictive, the binary similarity can be easily applied to any domain. 

\subsubsection{Hamming Distance}
The Hamming distance between two strings counts the number of positions where different elements occur. 

\begin{equation}
	d^{HAM}(x,y) = 
\end{equation}
As a constraint, the arrays have to be off the same length. 
This constraint can be easily overcome by counting the empty places in the shorter array. 

Hamming distance relies on very strict matching. This results in failure to recognize sub-array alignment (longest common parts). 
For example, given the strings \emph{"Evaluation"} and \emph{"Realization"}, 
the Hamming Distance is 9, 
since only the substring \emph{"al"} occurs at the same position at the two inputs.
\todo {Hamming distance example}

\subsubsection{Edit Distance}
The Edit Distance  is a measure for quantifying the dissimilarity between two vectors\cite{Jurafsky:2009:SLP:1214993}.
It is also known as \emph{Levenshtein Distance}, which was specifically defined for string comparison 
(also referred as \emph{String Edit Distance})\cite{Navarro:2001:GTA:375360.375365}.

The (String) Edit distance equals the number of operations required to transform array $x$ into array $y$. 
This operations are limited to:
\begin{itemize}
	\item[--] Insertion of single character
	\item[--] Substitution of single character
	\item[--] Deletion of single character
\end{itemize}

The formal expression of is given with equation \ref{eq:edit-distance}.
\begin{equation}
	d^{EDIT}(x,y) = 
	\label{eq:edit-distance}
\end{equation}

Edit Distance overcomes the drawbacks of the Hamming Distance. 
It is robust in terms of substrings alignment. 
This can be shown on the previous example, where the given strings are \emph{"Evaluation"} and \emph{"Realization"}.
First, the ED will identify the common substrings, which in this case are underlined in \todo{Reference Figure}. 
These symbols remain unmodified. 
The remaining operations needed to convert string Q to  S are:
\begin{itemize}
	\item[--] One substitution of character 'E' to 'R'
	\item[--] One substitution of character 'V' to 'E'
	\item[--] One substitution of character 'U' to 'I'
	\item[--] One insertion of character 'Z'
\end{itemize}

This results in total of 4 operation, giving the distance (or cost) of 4.

Assigning cost to each operation results into Weighted Edit Distance\cite{Chen:2004:MLE:1316689.1316758}.
\begin{equation}
	d^{WEDIT}(x,y) = 
\end{equation}

Both Edit Distance and the Weighted Edit Distance are metric.

\subsection{Example distance and similarity functions using graph structure}

\subsubsection{Neighborhood Random Walk Distance}

Based on the connectivity in a graph $G$, a distance measure between vertices can be defined. 
Two nodes $v_{a}$ and $v_{b}$ are considered close if there are multiple paths connecting them. 
If there are few or no paths, then the are considered dissimilar.

\begin{definition}
	Given $l$ as the upper bound how far a random walker can go and the restart probability $c \in [0, 1]$, 
the Random Walk Distance between two nodes  $v_{i}$ and $v_{j}$ in graph $G$ is defined as:
\begin{equation}
	d^{NRW}(v_{a}, v_{b}) = \sum_{\tau:v_{i}\rightsquigarrow v_{j}}p(\tau)c(1-c)^{len(\tau)}
\end{equation}
defined for all paths $\tau$ between $v_{a}$ and $v_{b}$ whose length is shorter than $l$ with transition probability p($\tau$). 
\end{definition}

For unweighted graph, the probability distribution of edges considered in the final path is uniform.

\subsubsection{SimRank}
SimRank calculates similarities between vertices in graph $G$ based on their structural context\cite{Jeh02simrank:a}.
The underlying idea of this algorithm is that two objects are similar if they are referenced by similar other objects. 
The objects are modeled as vertices (nodes) and the relations between them as directed edges.
This makes SimRank domain independent, applicable for every domain that can be modeled using the basic directed graph model.


SimRank is defined as:
\begin{equation}
	s^{SR}(v_{a},v_{b}) = \dfrac{C}{\arrowvert I(v_{a})\arrowvert \arrowvert I(v_{b})\arrowvert}
	\sum_{i=1}^{\arrowvert I(v_{a})\arrowvert}\sum_{j=1}^{\arrowvert I(v_{b})}s^{SR}(I_{i}(v_{a}), I_{j}(v_{b}))
	\label{eq:simrank}
\end{equation}
where $C \in [0,1]$ is a the factor of decay, $I(v)$ is the set of predecessor nodes for a given node $v$, 
and $\arrowvert I(v)\arrowvert$ is the size of that set.
Dividing by the total number of predecessors pairs allows us to obtain normalized value: 
a range between 0 (maximum dissimilarity)  to 1 (same pair of nodes). 

For veticies that have no predecessors ($I(v)=\emptyset$), the similarity is set to zero.
Alternatively, equation \ref{eq:simrank} can be expressed using the set of successors $O(v))$ of the node $v$,
or combining the result from both approaches.

As stated, the intuition behind this approach is recursive - 
in order to define similarity between two objects, we need to rely on already pre-calculated similarity between the neighbors.
SimRank is computing by iteratively evaluating over each pair of nodes $(v_{a},v_{b})$ until fixpoint is reached. 
The initial similarity score is  calculated using the binary similarity (equation \ref{eq:binary}):

\begin{equation}
	s_{0}^{SR}(v_{a},v_{b}) = \begin{cases}
	1  & \quad \text{if } v_{a}=v_{b} \\
	0  & \quad \text{else }\\
	\end{cases}
\end{equation}
The node pairs, where the $s_{0}^{SR}(v_{a},v_{b})=1$ are called singleton pairs.
For each iteration $k$, the similarity score $s_{k+1}^{SR}$ is updated using $s_{k+1}^{SR}$:

\begin{equation}
	s_{k+1}^{SR}(v_{a},v_{b})= \dfrac{C}{\arrowvert I(v_{a})\arrowvert \arrowvert I(v_{b})\arrowvert}
	\sum_{i=1}^{\arrowvert I(v_{a})\arrowvert}\sum_{j=1}^{\arrowvert I(v_{b})}s_{k}^{SR}(I_{i}(v_{a}), I_{j}(v_{b}))
\end{equation}
where $v_{a}\neq v_{b}$ (we only update the score for non-singleton pairs).

The end result after $K$ iterations is a graph $G^{2} = (V^{2},E^{2})$, where the nodes are pairs from $G$
and the edges are aggregated for both elements of the vertex pair:
\begin{itemize}
	\item[--] $V^{2} = V\times V$ are vertex pairs in $V$
	\item[--] $E^{2}:(a,b) \rightarrow (c,d)$ in $G^{2}$ iff $a\rightarrow c$ and $b\rightarrow d$ in $G$
\end{itemize}

\todo{Figure}{Figure for SimRank and end result}

\section{Association rule mining}
Association rule mining searches for recurring relationships in dataset.
More specific, it discovers the associations and correlations among items.
\subsection{Background}
Let $I=\{I1,I2,..,Im\}$ be n set of one or more items (also referred as itemset).
An itemset of length k is labeled as k-itemset.

Let $D$ be the database of transactions where each transaction is a non-empty itemset such that $T \subseteq I$.
Transaction $T$ is said to contain an itemset $A$  if $A\subseteq T$.
The goal of association rule mining is given the set of transactions $T$, 
find association rules that will predict the occurrence of an item $I$ based on the occurrences of other items in the transaction $T$.

An association rule indicated as $A\Rightarrow B$, where $A$ and $B$ are non-empty itemsets.